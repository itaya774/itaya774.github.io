<!DOCTYPE html>
<html lang="ja">
<head>
<meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<title>Hidenori Itaya' pages</title>
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="description" content="ここにサイト説明を入れます">
<meta name="keywords" content="キーワード１,キーワード２,キーワード３,キーワード４,キーワード５">
<link rel="stylesheet" href="../css/style.css">
<link rel="stylesheet" href="../css/print.css" media="print">
<script src="js/openclose.js"></script>
<!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<style>
.menu1 a {background-position: -10px -10px;}
.menu2 a {background-position: -10px -130px;}
.menu3 a {background-position: -10px -250px;}
.menu4 a {background-position: -10px -370px;}
.menu5 a {background-position: -10px -490px;}
</style>
<![endif]-->
<script type="text/javascript"
  src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    TeX: { equationNumbers: { autoNumber: "AMS" }},
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    },
    "HTML-CSS": { matchFontHeight: false },
    displayAlign: "left",
    displayIndent: "2em"
  });
</script>
</head>


<body class="link">

<div id="container">

<!--PC用（801px以上端末）で表示させるブロック-->
<header class="pc">

	<!--PC用（801px以上端末）メニュー-->
	<nav id="menubar">
	<ul>
		<!--<li class="menuimg menu1 current"><a href="index.html"><span>Home</span></a></li>-->
		<li class="menuimg menu2"><a href="../index.html"><span>Profile</span></a></li>
		<li class="menuimg menu3"><a href="../research.html"><span>Research</span></a></li>
		<!--<li class="menuimg menu4"><a href="works.html"><span>Wroks</span></a></li>-->
		<li class="menuimg menu4"><a href="../link.html"><span>Links</span></a></li>
		<!--<li class="menuimg menu5"><a href="contact.html"><span>Contact</span></a></li>-->
	</ul>
	</nav>
	<ul class="icon">
		<!--<li><a href="#"><img src="images/icon_facebook.png" alt="Facebook"></a></li>-->
		<li><a href="https://twitter.com/ita03395320"><img src="../images/icon_twitter.png" alt="Twitter"></a></li>
		<li><a href="https://github.com/itaya774"><img src="../images/icon_github.png" alt="Github"></a></li>
		<!--<li><a href="#"><img src="images/icon_instagram.png" alt="Instagram"></a></li>-->
		<!--<li><a href="#"><img src="images/icon_youtube.png" alt="YouTube"></a></li>-->
	</ul>

</header>
<!--/.pc-->

<!--小さな端末用（800px以下端末）で表示させるブロック-->
<header class="sh">

	<!--小さな端末用（800px以下端末）メニュー-->
	<div id="menubar-s">
	<nav>
	<ul>
		<!--<li class="menuimg menu1 current"><a href="index.html"><span>Home</span></a></li>-->
		<li class="menuimg menu2"><a href="../index.html"><span>Profile</span></a></li>
		<li class="menuimg menu3"><a href="../research.html"><span>Research</span></a></li>
		<!--<li class="menuimg menu4"><a href="works.html"><span>Wroks</span></a></li>-->
		<li class="menuimg menu4"><a href="../link.html"><span>Links</span></a></li>
		<!--<li class="menuimg menu5"><a href="contact.html"><span>Contact</span></a></li>-->
	</ul>
	</nav>
	<ul class="icon">
		<!--<li><a href="#"><img src="images/icon_facebook.png" alt="Facebook"></a></li>-->
		<li><a href="https://twitter.com/ita03395320"><img src="../images/icon_twitter.png" alt="Twitter"></a></li>
		<li><a href="https://github.com/itaya774"><img src="../images/icon_github.png" alt="Github"></a></li>
		<!--<li><a href="#"><img src="images/icon_instagram.png" alt="Instagram"></a></li>-->
		<!--<li><a href="#"><img src="images/icon_youtube.png" alt="YouTube"></a></li>-->
	</ul>
	
	</div>
	<!--/#menubar-s-->

</header>
<!--/.sh-->

<div id="contents">

<div id="main">

<section id="pagetop">

<h2 class="title">Links<span>リンク</span></h2>

<section class="box">
<h2>Deep Q-Network (DQN)</h2>
<font size="3">
Q学習とDeep Convolutional Neural Network (DCNN)を組み合わせた手法．<br>
Q学習では Q-table を用いて行動価値を表現していたが，DQN ではニューラルネットワークを用いた演算により行動価値を近似している．
ネットワークの出力が最適な行動価値と同様になるように学習，すなわちネットワークのパラメータを更新する．<br>
誤差関数 $L_{\theta}$<br>
\[
L_{\theta} = E\biggl[\frac{1}{2}\Bigl(r+\gamma\max_{a'}Q_{\theta_{i}}(s',a')-Q_{\theta_{i}}(s,a)\Bigr)^{2}\biggr]
\]
$\theta$はネットワークの重み, $r+\gamma\max_{a'}Q_{\theta_{i}}(s',a')$は教師あり学習での教師データにあたり，targetと呼ぶ．
また，誤差逆伝播する際の勾配は微分することで得られる．
勾配 $∇L(\theta_{i})$
\[
∇L(\theta_{i}) = E\biggl[\Bigl(r+\gamma\max_{a'}Q_{\theta_{i}}(s',a')-Q_{\theta_{i}}(s,a)\Bigr)∇Q_{\theta_{i}}(s,a)\biggr]
\]
<br>
DQNでの3つの工夫<br>
・<u>Experience replay</u><br>
	<p>
		環境とのインタラクションによって獲得した経験をメモリに蓄積し，学習の際はそのメモリから経験をランダムサンプリングする工夫．
		ここでの経験とは，状態と行動，報酬，次状態を1セットにしたものである．
		強化学習では，経験は連続的であり，獲得した順序により相関が生じる．
		そのため，Experience replayを用いることで，相関を解消することでき学習を安定させている．
	</p>
・<u>Target Q-network</u><br>
	<p>
		誤差関数$L_{\theta}$と勾配$∇L(\theta_{i})$において，targetを逐次的に更新するのではなく，数ステップ毎に更新する工夫．
		DQNにおけるtargetは，前パラメータ$\theta_{i−1}$を用いて計算する．
		しかし，学習毎に$\theta_{i−1}を更新すると，targetが学習毎に変わるため，targetへの近似が困難になるという問題点が存在する．
		そのため，Target Q-networkを用いることで，この問題を解決している．
	</p>
・<u>Clipping reward</u><br>
	<p>
		報酬値のスケールを統一する工夫．
		強化学習では報酬を頼りに最適な行動を学習するため，問題設定毎に報酬を設計する必要がある．
		このように報酬設計は問題設定に依存するため，スケールに大きな差が出る．
		そのため，問題設定毎に最適なハイパーパラメータが異なり，報酬設計によっては収束が困難になる場合がある．
		そこで，正の報酬は+1，報酬なしは0，負の報酬は−1に統一する．
		このClipping rewardにより，様々な問題設定間で学習しやすくしている．
	</p>


</font>
<p></p>
</section>

<footer>
<small>Copyright&copy; <a href="../index.html">SAMPLE SITE</a> All Rights Reserved.</small>
</footer>

</div>
<!--/#main-->

</div>
<!--/#contents-->

</div>
<!--/#container-->

<!--ページの上部に戻る「↑」ボタン-->
<p class="nav-fix-pos-pagetop"><a href="#pagetop">↑</a></p>

<!--メニュー開閉ボタン-->
<div id="menubar_hdr" class="close"></div>

<!--メニューの開閉処理条件設定　800px以下-->
<script>
if (OCwindowWidth() <= 800) {
	open_close("menubar_hdr", "menubar-s");
}
</script>

</body>
</html>
